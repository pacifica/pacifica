{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notifications Event Workflow\n",
    "\n",
    "Pacifica does have some basic event processing libraries to manage running processes on data present in Pacifica.\n",
    "\n",
    "## Imports and Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install eventlet pacifica-metadata==0.12.4 pacifica-policy==0.8.2 pacifica-cli==0.5.0 pacifica-downloader==0.4.0 pacifica-uploader==0.3.0 pacifica-dispatcher==0.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import contextlib\n",
    "import hashlib\n",
    "import tempfile\n",
    "import threading\n",
    "from io import StringIO\n",
    "\n",
    "import requests\n",
    "import cherrypy\n",
    "import playhouse.db_url\n",
    "from cloudevents.model import Event\n",
    "from jsonpath2.path import Path\n",
    "from celery.utils.log import mlevel\n",
    "\n",
    "from pacifica.downloader import Downloader\n",
    "from pacifica.uploader import Uploader\n",
    "\n",
    "from pacifica.dispatcher.models import File, Transaction, TransactionKeyValue\n",
    "from pacifica.dispatcher.event_handlers import EventHandler\n",
    "from pacifica.dispatcher.receiver import create_peewee_model\n",
    "from pacifica.dispatcher.router import Router\n",
    "from pacifica.dispatcher.downloader_runners import DownloaderRunner, RemoteDownloaderRunner\n",
    "from pacifica.dispatcher.uploader_runners import UploaderRunner, RemoteUploaderRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Test Data Set\n",
    "\n",
    "The test data set is coupled with the metadata service and\n",
    "can be loaded through docker like the following.\n",
    "\n",
    "NOTE: If you've already done the following by running the Examples notebook you must skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_data.loadit_test import main\n",
    "\n",
    "os.environ['METADATA_URL'] = 'http://metadataserver:8121'\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The UniqueID Interface\n",
    "\n",
    "The uniqueid interface needs to be updated with the new files we\n",
    "inserted prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in ['file', 'upload_job']:\n",
    "    resp = requests.get('http://uniqueid:8051/getid', params={'mode': mode, 'range': '200'})\n",
    "    assert resp.status_code == 200\n",
    "    print(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a Subscription\n",
    "\n",
    "We need to register for events and tell the notifications service where to send the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.post(\n",
    "    'http://notifyfrontend:8070/eventmatch',\n",
    "    headers={'Http-Remote-User': 'dmlb2001'},\n",
    "    json={\n",
    "        \"name\": \"My Event Match\",\n",
    "        \"jsonpath\": \"\"\"\n",
    "            $[?(\n",
    "                @[\"cloudEventsVersion\"] = \"0.1\" and\n",
    "                @[\"eventType\"] = \"org.pacifica.metadata.ingest\"\n",
    "            )]\n",
    "        \"\"\",\n",
    "        \"target_url\": \"http://jupyter:8080/receive\"\n",
    "    }\n",
    ")\n",
    "assert resp.status_code == 200\n",
    "print(resp.json())\n",
    "subscription_uuid = resp.json()['uuid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Dispatcher\n",
    "\n",
    "The dispatcher has several steps to setup. First, we need to create the database. Then we'll setup a sample event handler class to work on the event. We'll then setup the CherryPy web service to receive events. Then we'll setup the Celery workers to handle the work.\n",
    "\n",
    "### Setup Constants and Create Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_ = playhouse.db_url.connect(os.getenv('DATABASE_URL', 'postgres://jupyter:jupyter@jupyterdb/jupyter'))\n",
    "\n",
    "ReceiveTaskModel = create_peewee_model(DB_)\n",
    "\n",
    "MODELS_ = (ReceiveTaskModel, )\n",
    "\n",
    "DB_.create_tables(MODELS_)\n",
    "\n",
    "ROUTER = Router()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Simple Event Handler\n",
    "\n",
    "The event handler is a class that implements the `pacifica.dispatcher.event_handler.EventHandler` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEventHandler(EventHandler):\n",
    "    def __init__(self, downloader_runner: DownloaderRunner, uploader_runner: UploaderRunner) -> None:\n",
    "        \"\"\"Save the download and upload runner classes for later use.\"\"\"\n",
    "        super(SimpleEventHandler, self).__init__()\n",
    "        self.downloader_runner = downloader_runner\n",
    "        self.uploader_runner = uploader_runner\n",
    "        \n",
    "    def handle(self, event: Event) -> None:\n",
    "        \"\"\"\n",
    "        Example handle event.\n",
    "        \n",
    "        This handler downloads all files in the event.\n",
    "        Converts the files to uppercase and uploads them back to Pacifica.\n",
    "        \"\"\"\n",
    "        transaction_inst = Transaction.from_cloudevents_model(event)\n",
    "        transaction_key_value_insts = TransactionKeyValue.from_cloudevents_model(event)\n",
    "        file_insts = File.from_cloudevents_model(event)\n",
    "        with tempfile.TemporaryDirectory() as downloader_tempdir_name:\n",
    "            with tempfile.TemporaryDirectory() as uploader_tempdir_name:\n",
    "                for file_opener in self.downloader_runner.download(downloader_tempdir_name, file_insts):\n",
    "                    with file_opener() as file_fd:\n",
    "                        with open(os.path.join(uploader_tempdir_name, file_fd.name), 'w') as wfile_fd:\n",
    "                            wfile_fd.write(file_fd.read().upper())\n",
    "                (_bundle, _job_id, _state) = self.uploader_runner.upload(\n",
    "                    uploader_tempdir_name, transaction=Transaction(\n",
    "                        submitter=transaction_inst.submitter,\n",
    "                        instrument=transaction_inst.instrument,\n",
    "                        project=transaction_inst.project\n",
    "                    ), transaction_key_values=[\n",
    "                        TransactionKeyValue(key='uppercase_text', value='True'),\n",
    "                        TransactionKeyValue(key='Transactions._id', value=transaction_inst._id)\n",
    "                    ]\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link Event Handler\n",
    "\n",
    "We need to link up the SimpleEventHandler with the remote downloader and uploader. After that we should setup the Celery worker and CherryPy application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CARTD_ADDR'] = 'cartfrontend'\n",
    "os.environ['INGEST_ADDR'] = 'ingestfrontend'\n",
    "os.environ['POLICY_ADDR'] = 'policyserver'\n",
    "\n",
    "ROUTER.add_route(\n",
    "    Path.parse_str(\"\"\"\n",
    "        $[\"data\"][*][?(\n",
    "            @[\"destinationTable\"] = \"TransactionKeyValue\" and\n",
    "            @[\"key\"] = \"uppercase_text\" and\n",
    "            @[\"value\"] = \"False\"\n",
    "          )]\n",
    "    \"\"\"),\n",
    "    SimpleEventHandler(\n",
    "        RemoteDownloaderRunner(Downloader()), RemoteUploaderRunner(Uploader())\n",
    "    )\n",
    ")\n",
    "\n",
    "CELERY_APP = ReceiveTaskModel.create_celery_app(\n",
    "    ROUTER,\n",
    "    'pacifica.dispatcher.app',\n",
    "    'pacifica.dispatcher.tasks.receive',\n",
    "    backend='rpc://',\n",
    "    broker='pyamqp://guest:guest@jupyteramqp:5672//'\n",
    ")\n",
    "\n",
    "APPLICATION = ReceiveTaskModel.create_cherrypy_app(CELERY_APP.tasks['pacifica.dispatcher.tasks.receive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start CherryPy Server\n",
    "\n",
    "This should start the CherryPy server in a thread and give control back to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cherrypy.tree.mount(APPLICATION)\n",
    "cherrypy.config.update({'server.socket_host': '0.0.0.0'})\n",
    "cherrypy.engine.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Celery Application\n",
    "\n",
    "We are going to start the celery application in a separate thread in solo mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_celery_worker():\n",
    "    celery_workers = CELERY_APP.Worker(pool_cls='solo', loglevel=mlevel('debug'))\n",
    "    celery_workers.start()\n",
    "    return celery_workers.exitcode\n",
    "    \n",
    "celery_worker = threading.Thread(target=run_celery_worker)\n",
    "celery_worker.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigger Workflow with Upload\n",
    "\n",
    "To trigger the processes we need to initiate everything with an upload. First, we need to configure the uploader by reading the original uploader configuration and adding our trigger key value pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('pacifica-cli', 'uploader.json'), 'r') as uploader_fd:\n",
    "    uploader_data = json.loads(uploader_fd.read())\n",
    "uploader_data.append({\n",
    "    \"destinationTable\": \"TransactionKeyValue\",\n",
    "    \"key\": \"uppercase_text\",\n",
    "    \"metaID\": \"uppercase-tkv\",\n",
    "    \"query_results\": [],\n",
    "    \"value\": \"False\"\n",
    "})\n",
    "with open(os.path.join('pacifica-cli', 'uploader-notify.json'), 'w') as uploader_fd:\n",
    "    uploader_fd.write(json.dumps(uploader_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we configure the rest of the uploader environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['UPLOAD_URL'] = 'http://ingestfrontend:8066/upload'\n",
    "os.environ['UPLOAD_STATUS_URL'] = 'http://ingestfrontend:8066/get_state'\n",
    "os.environ['UPLOAD_POLICY_URL'] = 'http://policyserver:8181/uploader'\n",
    "os.environ['UPLOAD_VALIDATION_URL'] = 'http://policyserver:8181/ingest'\n",
    "os.environ['DOWNLOAD_URL'] = 'http://cartfrontend:8081'\n",
    "os.environ['DOWNLOAD_POLICY_URL'] = 'http://policyserver:8181/status/transactions/by_id'\n",
    "os.environ['AUTHENTICATION_TYPE'] = 'None'\n",
    "os.environ['UPLOADER_CONFIG'] = os.path.join('pacifica-cli', 'uploader-notify.json')\n",
    "from pacifica.cli.__main__ import main\n",
    "sys.argv = ['cli', 'configure']\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pacifica.cli.__main__ import main\n",
    "stdout_buf = StringIO()\n",
    "stderr_buf = StringIO()\n",
    "with contextlib.redirect_stderr(stderr_buf):\n",
    "    with contextlib.redirect_stdout(stdout_buf):\n",
    "        sys.argv = ['cli', 'upload', '--logon=10', 'test_data']\n",
    "        main()\n",
    "json_obj_start = stdout_buf.getvalue().find('{')\n",
    "json_obj_end = stdout_buf.getvalue().find('}')\n",
    "print(stdout_buf.getvalue()[json_obj_start:json_obj_end+1])\n",
    "job_id = json.loads(stdout_buf.getvalue()[json_obj_start:json_obj_end+1])['job_id']\n",
    "print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification\n",
    "\n",
    "Make sure the notification didn't barf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get('http://notifyfrontend:8070/eventmatch/{}'.format(subscription_uuid),\n",
    "    headers={'Http-Remote-User': 'dmlb2001'})\n",
    "assert resp.status_code == 200\n",
    "assert resp.json()['error'] is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispatcher Checks\n",
    "\n",
    "We should also make sure the dispatcher ran successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get('http://127.0.0.1:8080')\n",
    "assert resp.status_code == 200\n",
    "assert resp.json()\n",
    "\n",
    "# the first one is more recent and represents a second event from the uppercase upload.\n",
    "resp = requests.get('http://127.0.0.1:8080/get/{}'.format(resp.json()[1]['taskID']))\n",
    "assert resp.status_code == 200\n",
    "assert resp.json()['taskStatus'] == '200 OK'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}